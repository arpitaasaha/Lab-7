{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arpitaasaha/Lab-7/blob/main/CSE366_lab_7_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zwFnJsE6vjf8",
        "outputId": "a1ef39f2-bf75-4851-8087-998172e37fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(\n",
        "    root=r'/content/drive/MyDrive/CSE366/caltech-101/__MACOSX/caltech-101/101_ObjectCategories/101_ObjectCategories',\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "b3CbgfkflxJ2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "train_data, val_data, test_data = random_split(dataset, [train_size,\n",
        "val_size, test_size])"
      ],
      "metadata": {
        "id": "R3wHpnHrh1FS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "8BuqmLKjh2CB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import vgg19\n",
        "model = vgg19(pretrained=True)\n",
        "model.classifier[6] = torch.nn.Linear(4096, 101) # Adjust the final layer"
      ],
      "metadata": {
        "id": "hyLT0SJPh59r",
        "outputId": "5220fcf8-b0a9-4e2e-9f7d-f3a7f7a09db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:07<00:00, 73.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet50\n",
        "model = resnet50(pretrained=True)\n",
        "\n",
        "model.fc = torch.nn.Linear(2048, 101) # Adjust the final layer for 101"
      ],
      "metadata": {
        "id": "8O47tumOh6Ot",
        "outputId": "0ec10cfd-2ce5-411b-9c33-e5db6d727f7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 131MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import efficientnet_b0\n",
        "model = efficientnet_b0(pretrained=True)\n",
        "model.classifier[1] = torch.nn.Linear(1280, 101) # Adjust the final layer"
      ],
      "metadata": {
        "id": "o4IBMdDEiCrm",
        "outputId": "034315a5-a83b-4253-f37c-9dfc48565bf1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 80.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "BWo3I0j3iJ1t"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "model.to(device)\n",
        "\n",
        "# Example: Move a tensor to the selected device\n",
        "tensor = torch.randn(10).to(device)\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "niHt450niQAf",
        "outputId": "4ef54fca-bd60-4f34-f64b-be51aa6eab50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        labels = labels % 101\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "KvK1r8fdiQ99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        # Move images and labels to the same device as the model\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # Compute validation metrics"
      ],
      "metadata": {
        "id": "hWhIk9MjiTu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "uVmO_Px4icsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "\t'lr': [0.1, 0.01, 0.001],\n",
        "\t'batch_size': [16, 32, 64]\n",
        "}\n",
        "\n",
        "best_params = None\n",
        "best_accuracy = 0\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
        "\ttrain_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True)\n",
        "\n",
        "\t# Perform training and validation here\n",
        "\t# For demonstration, let's assume accuracy is calculated and assigned\n",
        "\taccuracy = 0.85  # Replace with actual accuracy calculation\n",
        "\n",
        "\tif accuracy > best_accuracy:\n",
        "\t\tbest_accuracy = accuracy\n",
        "\t\tbest_params = params\n",
        "\n",
        "print(f\"Best Params: {best_params}\")"
      ],
      "metadata": {
        "id": "EyluWsLkidYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images and labels to the same device as the model\n",
        "        images = images.to(device)  # Move images to device\n",
        "        labels = labels.to(device)  # Move labels to device\n",
        "        outputs = model(images)\n",
        "        # Compute test metrics"
      ],
      "metadata": {
        "id": "6zgFRwLsijrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "y_pred = []\n",
        "y_true = []\n",
        "with torch.no_grad():\n",
        "\tfor images, labels in test_loader:\n",
        "\t\t# Move images and labels to the same device as the model\n",
        "\t\timages = images.to(device)  # Move images to device\n",
        "\t\tlabels = labels.to(device)  # Move labels to device # Added this line\n",
        "\t\toutputs = model(images)\n",
        "\t\t_, preds = torch.max(outputs, 1)\n",
        "\t\ty_pred.extend(preds.cpu().numpy()) # Moved preds to CPU before converting to numpy\n",
        "\t\ty_true.extend(labels.cpu().numpy()) # Moved labels to CPU before converting to numpy\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "jfXvQXJUivWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = dataset.classes  # Define class_names based on the dataset\n",
        "\n",
        "# Ensure the number of class names matches the number of unique labels in y_true\n",
        "unique_labels = sorted(set(y_true))\n",
        "adjusted_class_names = [class_names[i] for i in unique_labels]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_true, y_pred, labels=unique_labels, target_names=adjusted_class_names))"
      ],
      "metadata": {
        "id": "XFkz2zlJiwD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_accuracy(output, target, k=5):\n",
        "\twith torch.no_grad():\n",
        "\t\tmax_k_preds = torch.topk(output, k, dim=1).indices\n",
        "\t\tcorrect = max_k_preds.eq(target.view(-1, 1).expand_as(max_k_preds))\n",
        "\t\treturn correct.any(dim=1).float().mean().item()"
      ],
      "metadata": {
        "id": "9Kh4vnRMizhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "for i, acc in enumerate(per_class_accuracy):\n",
        "\tprint(f\"Class {class_names[i]} Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "id": "dFavG126i4XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "wYTTI4Rhi4_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "features = []\n",
        "labels_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        # Move images to the same device as the model\n",
        "        images = images.to(device)  # Move images to device\n",
        "\n",
        "        output = model(images)\n",
        "        features.append(output)\n",
        "        labels_list.append(labels)\n",
        "\n",
        "features = torch.cat(features).cpu().numpy() # Move features to CPU before converting to numpy\n",
        "labels_list = torch.cat(labels_list).cpu().numpy() # Move labels_list to CPU before converting to numpy\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_features = tsne.fit_transform(features)\n",
        "plt.scatter(reduced_features[:, 0], reduced_features[:, 1], c=labels_list, cmap='tab10')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DCF7Ts-Ki8dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "id": "y5fPNvp4jAQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Adjust based on the model's architecture\n",
        "# Check model architecture and set target layer\n",
        "if isinstance(model, torch.nn.Sequential):  # VGG19\n",
        "    target_layer = model.features[-1]\n",
        "elif isinstance(model, torch.nn.Module) and hasattr(model, 'layer4'):  # ResNet50\n",
        "    target_layer = model.layer4[-1]\n",
        "elif isinstance(model, torch.nn.Module) and hasattr(model, 'features'):  # EfficientNet-B0\n",
        "    target_layer = model.features[-1]\n",
        "else:\n",
        "    raise ValueError(\"Model architecture not recognized\")\n",
        "\n",
        "# Initialize GradCAM\n",
        "cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "\n",
        "# Generate and display CAMs\n",
        "count = 0\n",
        "for images, labels in test_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    labels = labels % 101  # Ensure labels are within the correct range\n",
        "    targets = [ClassifierOutputTarget(label.item()) for label in labels]\n",
        "    grayscale_cam = cam(input_tensor=images, targets=targets)\n",
        "    for i in range(len(images)):\n",
        "        if count >= 5:\n",
        "            break\n",
        "        image = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        image = (image - image.min()) / (image.max() - image.min())  # Normalize to [0, 1]\n",
        "        cam_image = show_cam_on_image(image, grayscale_cam[i])\n",
        "        plt.imshow(cam_image)\n",
        "        plt.show()\n",
        "        count += 1\n",
        "    if count >= 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "dj9VrLu9jCs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zAsWD9r2jI8Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}